{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Meta-Learning-Shared-Hiearies\n",
    "\n",
    "В [статье](https://arxiv.org/pdf/1710.09767.pdf) рассматривается подход к обучению иерархический политики.\n",
    "Основная цель данного метода - быстрое дообучение политики под новую задачу (модель не видела их в процессе обучения).\n",
    "\n",
    "\n",
    "## Идея\n",
    "\n",
    "Модель имеет иерархическую структуру и состоит из *мастер политики* обучаемой отдельно для каждой задачи,\n",
    "и набора *подполитик*, общих для всего набора задач.\n",
    "\n",
    "Мастер политика рассматривается как политика над подполитиками, т.е. отвечает за переключение между подполитиками\n",
    "в процессе работы алгоритма.\n",
    "Подполитики, в свою очередь, отвечают за обучение некоторых примитивов работы, т.е. каждая полполитика отвечает за\n",
    "специфичный сценарий.\n",
    "\n",
    "\n",
    "## Формализация\n",
    "\n",
    "Задача рассматривается как марковси процесс $ P(s',r | s, a) $, где\n",
    "$s', s$ - следующее и текущее состояние, $a$ действие, $r$ - ревард на данном шаге.\n",
    "\n",
    "Есть распределение над задачами (над марковскики процессами) $P_{M}$.\n",
    "Агент описывается двумя наборами параметров: $\\theta, \\phi$, тогда политика агента $\\pi_{\\theta, \\phi}(a|s)$.\n",
    "\n",
    "Здесь\n",
    "* $\\theta$ - набор параметров мастер-политики, обучаемый заново для каждой задачи;\n",
    "* $\\phi$ - параметры подполитик, общие для всех для всех задач и обучаемые на наборе задач.\n",
    "\n",
    "Задача в *meta-learning* задаче - оптимизировать награду на протяжении жизни агента.\n",
    "Т.е в процессе обучения агента на выбранной задаче.\n",
    "\n",
    "\n",
    "## Архитектура\n",
    "\n",
    "В работе предлагается иерархическая структура политики.\n",
    "Общие параметры $\\phi = (\\phi_1, \\dots, \\phi_k)$, где каждый вектор параметров $\\phi_k$ соответствует отдельной\n",
    "подполитике $\\pi_{\\phi_k}(a|s)$.\n",
    "Вектор параметров $\\theta$ задает мастер-политику $\\pi_{\\theta}(a|s)$, задающую распределение над подполитиками.\n",
    "В предлагаемом метода переключение между подполитиками происходит каждые $N$ шагов $(0, N, 2N, \\dots)$.\n",
    "\n",
    "![Мотивирующая картинка: схема работы](../resources/motivation_picture_1.png)\n",
    "\n",
    "\n",
    "## Алгоритм обучения\n",
    "\n",
    "Предлагается итеративно учить множество подполитик, при этом уча на каждом итерации мастер-политику.\n",
    "Обучение на каждой $m ~ P_M$ происходит в два этапа:\n",
    "\n",
    "### Warmup\n",
    "\n",
    "Предварительное обучение мастер политики. На этом этапе учатся только параметры $\\theta$.\n",
    "Сыгранные шаги рассматриваются сгруппированными по $N$. То есть, действие - выбор подполитики, награда - суммарная\n",
    "награда за $N$ шагов.\n",
    "\n",
    "### Joint\n",
    "\n",
    "\n",
    "Совместное обучение мастер политики и подполитик. Делается $T$ шагов агента, затем оптимизируем $\\theta$ группируя шаги\n",
    "по $N$. Затем оптимизируем $\\phi$ обычным способом.\n",
    "\n",
    "Оптимизация проводилась с помощью A2C.\n",
    "\n",
    "\n",
    "## Эксперимент: WIP\n",
    "\n",
    "В ходе эксперимента проверялось:\n",
    "* возможность метода к обучению. для этого сравнивался график среднего ревардра на проэмлированной задаче для\n",
    "    обученного MLSH и для необученного MLSH (т.е. для каждой задачи тренируем заново.)\n",
    "* преимущество иерархического подхода перед одной shared политикой. Для этого сравнивался средний ревард для MLSH c\n",
    "     среднем ревардом для одной политики, обучаемой тем же способом, т.е. по задачам.\n",
    "\n",
    "Тестирование проводиться в средах Minigrid: DoorKey5x5, Empty, FourRoom.\n",
    "\n",
    "На **Графике 1** изображена зависимость среднего реварда от итерации обучения (номера просэмлированной задачи).\n",
    "По оси  $x$ изображен номер задачи*100 *(#TODO: надо поправить, извиняюсь, рудимент)*.\n",
    "На **Графике 2** изображен зависимость среднего реварда по 5 играм после каждой итерации обучения.\n",
    "\n",
    "Желтой линией обозначен MLSH, зеленой - Shared Policy, бежевой - необученная MLSH. Все графики построены для среды DoorKey.\n",
    "\n",
    "![График 1](../resources/mean_rewards.png)\n",
    "\n",
    "![График 2](../resources/seen_rewards.png)\n",
    "\n",
    "\n",
    "На текущем этапе эксперимент подтверждает только первый пункт: средний reward для MLSH растет.\n",
    "Похожие графики показываются на других небольших средах, но их не превожу, так как результаты не однозначные.\n",
    "При этом колебания награды при обучении очень велики.\n",
    "Возможно, это обусловлено выбранным методом обучения (A2C) и малым размером батча.\n",
    "Хотя, в случае обучения обычной политики в обычном режиме (не \"эпоха - одна задача\"), алгоритм сходиться.\n",
    "Далее планируется проверить ещё раз A2C на наличие ошибок, произвести эксперименты c PPO, как в оригинальной статье.\n",
    "\n",
    "\n",
    "## Other\n",
    "Запуски данных экспериментов на wandb:\n",
    "* [MLSH reset](https://app.wandb.ai/morgachev/mlsh/runs/2d4etdkz?workspace=user-morgachev)\n",
    "* [MLSH](https://app.wandb.ai/morgachev/mlsh/runs/2jeevlst?workspace=user-morgachev)\n",
    "* [Shared Policy](https://app.wandb.ai/morgachev/mlsh/runs/2vi3styx?workspace=user-morgachev)\n",
    "\n",
    "\n",
    "Соответствующие ноутбуки лежат в репозитории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import wandb\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "from torch import nn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from src import utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_name = \"MiniGrid-DoorKey-5x5-v0\"\n",
    "# env_name = \"MiniGrid-Empty-Random-5x5-v0\"\n",
    "# env_name = \"MiniGrid-DoorKey-8x8-v0\"\n",
    "env = utils.make_env(env_name)\n",
    "\n",
    "obs_space_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.a2c import A2CAlgo\n",
    "\n",
    "config = {\n",
    "    \"max_reward\": 0.99,\n",
    "    \"device\": \"cpu\",\n",
    "    \"env\": env_name,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"emb_dim\": 128,\n",
    "    \"n_env\": 8,\n",
    "    \"gamma\": 0.99,\n",
    "\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"lr\": 0.001,\n",
    "    \"value_loss_coef\": 0.5,\n",
    "    \"entropy_coef\": 0.01,\n",
    "\n",
    "    \"n_sub\": 4,\n",
    "    \"sub_n_iter\": 100,\n",
    "    \"sub_n_steps\": 3,\n",
    "    \"sub_lr\": 1e-4,\n",
    "\n",
    "    \"master_n_iter\": 30,\n",
    "    \"master_step_size\": 3,\n",
    "    \"master_n_steps\": 3,\n",
    "    \"master_lr\": 1e-3,\n",
    "\n",
    "    \"n_iter_epoch\": 50,\n",
    "    \"n_steps_sub\": 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
    "from src.mlsh_model import MLSHAgent\n",
    "from src.env_pool import MLSHPool\n",
    "\n",
    "agent = MLSHAgent(\n",
    "    config[\"n_sub\"],\n",
    "    n_actions,\n",
    "    obs_space_shape[1]\n",
    ")\n",
    "for p in agent.parameters():\n",
    "    nn.init.uniform_(p, -0.1, 0.1)\n",
    "\n",
    "pool = MLSHPool(agent,\n",
    "                lambda : utils.make_env(env_name),\n",
    "                config[\"n_env\"],\n",
    "                random_reset=False)\n",
    "\n",
    "wandb.init(project=\"mlsh\",\n",
    "           monitor_gym=True,\n",
    "           name=f\"mlsh_{env_name[9:]}+{config['n_sub']}_fixed\",\n",
    "           config=config,\n",
    "           dir=\"..\",\n",
    "           magic=True,\n",
    "           group=\"tests\")\n",
    "wandb.watch(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a2c_subpolicies = \\\n",
    "    A2CAlgo(agent.subpolicies.parameters(),\n",
    "            config[\"device\"],\n",
    "            n_actions,\n",
    "            config[\"gamma\"],\n",
    "            config[\"max_grad_norm\"],\n",
    "            config[\"entropy_coef\"],\n",
    "            config[\"sub_lr\"],\n",
    "            config[\"value_loss_coef\"])\n",
    "\n",
    "ac2_master = \\\n",
    "    A2CAlgo(list(agent.master_policy.parameters()),\n",
    "            config[\"device\"],\n",
    "            config[\"n_sub\"],\n",
    "            config[\"gamma\"],\n",
    "            config[\"max_grad_norm\"],\n",
    "            config[\"entropy_coef\"],\n",
    "            config[\"master_lr\"],\n",
    "            config[\"value_loss_coef\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 245/300 [2:24:47<29:29, 32.17s/it]"
     ]
    }
   ],
   "source": [
    "from src import mlsh_algo\n",
    "for i in tqdm(range(300)):\n",
    "    pool.update_seeds()\n",
    "    for seed, env in zip(pool.seeds, pool.envs):\n",
    "        env.seed(seed)\n",
    "        env.reset()\n",
    "\n",
    "    for p in agent.master_policy.parameters():\n",
    "        nn.init.uniform_(p, -0.1, 0.1)\n",
    "\n",
    "    mlsh_algo.warmup(ac2_master, pool,\n",
    "                     config[\"master_n_iter\"],\n",
    "                     config[\"master_step_size\"],\n",
    "                     config[\"master_n_steps\"],\n",
    "                     config[\"n_env\"])\n",
    "    epoch_rew = mlsh_algo.joint_train(\n",
    "        ac2_master,\n",
    "        a2c_subpolicies,\n",
    "        pool,\n",
    "        config[\"sub_n_iter\"],\n",
    "        config[\"master_step_size\"],\n",
    "        config[\"sub_n_steps\"],\n",
    "        config[\"n_env\"])[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        wandb.log({\n",
    "            \"mean_rewards_epoch\": epoch_rew,\n",
    "            \"seen_evaluate_reward\":\n",
    "                np.mean(utils.evaluate_mlsh(agent, env, 5,\n",
    "                    config[\"master_step_size\"],\n",
    "                    last_env=pool.seeds[0])[0]),\n",
    "            \"unseen_evaluate_reward\":\n",
    "                np.mean(utils.evaluate_mlsh(agent, env, 5,\n",
    "                    config[\"master_step_size\"],\n",
    "                    last_env=None)[0])\n",
    "        })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLSH",
   "language": "python",
   "name": "mlsh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}